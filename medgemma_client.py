#!/usr/bin/env python3
"""
MedGemma Client - Google's Medical AI Model
Direct integration with MedGemma for medical image analysis
"""

import os
import torch
import time
from typing import List, Dict, Any, Optional
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import config
from progress_logger import (
    show_step, show_success, show_error, show_info, show_warning, 
    start_progress, update_progress, complete_progress, 
    log_to_file, suppress_prints
)

class MedGemmaClient:
    """Client for Google's MedGemma medical AI model"""
    
    def __init__(self):
        """Initialize MedGemma client"""
        self.model_name = "google/medgemma-4b-it"
        self.device = self._get_device()
        self.processor = None
        self.model = None
        self.token = None
        
        # Load model with progress tracking
        self._initialize_model()
    
    def _initialize_model(self):
        """Initialize MedGemma model with progress tracking"""
        try:
            show_step("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MedGemma –∫–ª–∏–µ–Ω—Ç–∞")
            log_to_file("Starting MedGemma client initialization")
            
            # Setup device and memory
            self._setup_device()
            self._setup_cuda_environment()
            self._check_gpu_memory()
            self._setup_huggingface_token()
            
            # Load model with suppressed prints
            with suppress_prints():
                self._load_model()
            
            show_success("MedGemma –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —É—Å–ø–µ—à–Ω–æ")
            log_to_file("MedGemma client initialized successfully")
            
        except Exception as e:
            show_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ MedGemma: {e}")
            log_to_file(f"MedGemma initialization error: {e}", "ERROR")
            raise
    
    def _get_device(self) -> str:
        """Determine the best available device"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def _setup_device(self):
        """Setup device and clear cache"""
        self.device = self._get_device()
        show_info(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        log_to_file(f"Device: {self.device}")
        
        if self.device == "cuda":
            torch.cuda.empty_cache()
            show_info("üßπ CUDA –∫—ç—à –æ—á–∏—â–µ–Ω")
            log_to_file("CUDA cache cleared")
    
    def _setup_cuda_environment(self):
        """Setup CUDA environment variables"""
        if self.device == "cuda":
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
            os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
            show_info("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ CUDA –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            log_to_file("CUDA environment variables set for stability")
    
    def _check_gpu_memory(self):
        """Check GPU memory availability"""
        if self.device == "cuda" and torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            gpu_memory_free = torch.cuda.memory_reserved(0) / 1024**3
            show_info(f"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f}GB –æ–±—â–∞—è, {gpu_memory_free:.1f}GB —Å–≤–æ–±–æ–¥–Ω–∞—è")
            log_to_file(f"GPU memory: {gpu_memory:.1f}GB total, {gpu_memory_free:.1f}GB free")
            
            if gpu_memory < 8:
                show_warning("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–∞–ª–æ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è MedGemma. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 8GB")
                log_to_file("Warning: Low GPU memory for MedGemma. Minimum 8GB recommended", "WARNING")
    
    def _setup_huggingface_token(self):
        """Setup Hugging Face token"""
        from dotenv import load_dotenv
        from huggingface_hub import login
        load_dotenv()
        
        self.token = os.getenv("HUGGINGFACE_TOKEN")
        if self.token:
            show_success("‚úÖ –¢–æ–∫–µ–Ω Hugging Face –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
            log_to_file("Hugging Face token found in .env file")
            show_info(f"üîë –¢–æ–∫–µ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {self.token[:10]}...")
            log_to_file(f"Token starts with: {self.token[:10]}...")
            
            # –Ø–≤–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ HuggingFace Hub
            try:
                login(token=self.token, add_to_git_credential=False)
                show_success("‚úÖ –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ HuggingFace Hub —É—Å–ø–µ—à–Ω–∞")
                log_to_file("HuggingFace Hub authentication successful")
            except Exception as e:
                show_error(f"‚ùå –û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ HuggingFace Hub: {e}")
                log_to_file(f"HuggingFace Hub authentication error: {e}", "ERROR")
                raise
        else:
            show_error("‚ùå –¢–æ–∫–µ–Ω Hugging Face –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
            show_info("üí° –î–æ–±–∞–≤—å—Ç–µ HUGGINGFACE_TOKEN=your_token –≤ .env —Ñ–∞–π–ª")
            log_to_file("Hugging Face token not found in .env file", "ERROR")
            raise ValueError("–¢–æ–∫–µ–Ω Hugging Face –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MedGemma")
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"""
        try:
            show_step(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ MedGemma –º–æ–¥–µ–ª–∏: {self.model_name}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
            self.model = AutoModelForImageTextToText.from_pretrained(
                self.model_name,
                token=self.token,
                torch_dtype=torch.bfloat16 if self.device != "cpu" else torch.float32,
                device_map="auto" if self.device != "cpu" else None,
                trust_remote_code=True
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                token=self.token,
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            show_success(f"‚úÖ MedGemma –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            log_to_file(f"MedGemma model loaded successfully on {self.device}")
            
        except Exception as e:
            show_error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ MedGemma: {e}")
            log_to_file(f"MedGemma model loading error: {e}", "ERROR")
            raise
    
    def analyze_medical_image(self, image_data: Dict[str, Any], prompt: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —Å –ø–æ–º–æ—â—å—é MedGemma
        
        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å base64_image
            prompt: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        if not self.model or not self.processor:
            show_error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            log_to_file("Model not loaded for image analysis")
            return None
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ base64
            image_bytes = base64.b64decode(image_data['base64_image'])
            image = Image.open(BytesIO(image_bytes))
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if not prompt:
                prompt = """Analyze this medical CT image and provide a comprehensive medical assessment.

Please provide:

1. VISUAL FINDINGS:
   - Anatomical structures visible
   - Tissue densities and morphology
   - Any abnormal findings or pathology

2. CLINICAL INTERPRETATION:
   - Medical significance of findings
   - Differential diagnosis considerations
   - Severity assessment

3. RECOMMENDATIONS:
   - Additional imaging if needed
   - Clinical correlation required
   - Follow-up suggestions
   - Urgency level

Provide detailed, clinically relevant analysis focused on diagnostic and therapeutic implications."""
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are an expert radiologist specializing in CT image analysis."}]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image", "image": image}
                    ]
                }
            ]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True, 
                tokenize=True,
                return_dict=True, 
                return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16 if self.device != "cpu" else torch.float32)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è MedGemma
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.processor.tokenizer.pad_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            show_info("üîç –ü–û–õ–ù–´–ô –û–¢–í–ï–¢ MEDGEMMA (–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è):")
            log_to_file("Full MedGemma response (Image Analysis):")
            log_to_file("=" * 50)
            log_to_file(response)
            log_to_file("=" * 50)
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                torch.mps.empty_cache()
                
            return response.strip()
            
        except Exception as e:
            show_error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è MedGemma: {e}")
            log_to_file(f"MedGemma image analysis error: {e}", "ERROR")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                torch.mps.empty_cache()
                
            return None
    
    def analyze_ct_study(self, images: List[Dict[str, Any]], study_context: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ä–∏—é CT –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∫ –µ–¥–∏–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            study_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            
        Returns:
            –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        """
        if not images:
            return None
        
        show_info(f"üîç MedGemma –∞–Ω–∞–ª–∏–∑ CT –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ({len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)...")
        log_to_file(f"MedGemma CT study analysis ({len(images)} images)...")
        show_info("üè• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        log_to_file("Processing ALL images for comprehensive analysis")
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            individual_analyses = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            batch_size = 5  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è GPU
            total_batches = (len(images) + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(images))
                batch_images = images[start_idx:end_idx]
                
                show_info(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ {batch_idx + 1}/{total_batches} ({len(batch_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)...")
                log_to_file(f"Processing batch {batch_idx + 1}/{total_batches} ({len(batch_images)} images)...")
                
                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø–∞–∫–µ—Ç–∞
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                elif self.device == "mps":
                    torch.mps.empty_cache()
                
                for i, image_data in enumerate(batch_images):
                    global_idx = start_idx + i + 1
                    show_info(f"üìä –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {global_idx}/{len(images)}: ", end="")
                    log_to_file(f"Analyzing image {global_idx}/{len(images)}: ")
                    
                    slice_prompt = f"""Analyze this CT slice #{global_idx} from a medical study.

{f"Study Context: {study_context}" if study_context else ""}

Please provide:
1. Anatomical structures visible in this slice
2. Any pathological findings
3. Clinical significance of findings
4. Slice position and anatomical level

Focus on medically relevant observations. Be concise but thorough."""
                    
                    analysis = self.analyze_medical_image(image_data, slice_prompt)
                    
                    if analysis:
                        individual_analyses.append(f"=== CT SLICE {global_idx} ===\n{analysis}")
                        show_success("‚úÖ")
                        log_to_file(f"CT Slice {global_idx} analysis successful")
                    else:
                        show_error("‚ùå")
                        log_to_file(f"CT Slice {global_idx} analysis failed")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ GPU
                if batch_idx < total_batches - 1:
                    show_warning("‚è∏Ô∏è –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ GPU...")
                    log_to_file("‚è∏Ô∏è Pause between batches for GPU stability...")
                    time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –¥–ª—è GPU
            
            if not individual_analyses:
                return None
            
            show_info(f"üìã –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –æ—Ç—á—ë—Ç–∞ –∏–∑ {len(individual_analyses)} –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            log_to_file(f"Creating comprehensive study report from {len(individual_analyses)} analyzed images...")
            
            # –°–æ–∑–¥–∞—ë–º –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            study_summary = self.analyze_medical_text(
                f"""Based on the following comprehensive CT study analysis, provide a detailed radiology report:

ANALYZED SLICES: {len(individual_analyses)} of {len(images)} total images

{chr(10).join(individual_analyses)}

Study Context: {study_context}

Please provide a structured radiology report including:

1. TECHNIQUE AND QUALITY
2. FINDINGS BY ANATOMICAL REGION
3. IMPRESSION/CONCLUSION
4. RECOMMENDATIONS

Format as a professional radiology report with detailed findings.""",
                "Complete CT study comprehensive analysis"
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            final_report = f"""=== MEDGEMMA COMPLETE CT STUDY ANALYSIS ===

STUDY DETAILS:
- Total images in study: {len(images)}
- Images successfully analyzed: {len(individual_analyses)}
- Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- Model: MedGemma 4B (Google) - Vision + Text
- Analysis method: Direct image analysis
- Processing method: Batch processing for stability

=== INDIVIDUAL SLICE ANALYSES ===

{chr(10).join(individual_analyses)}

=== COMPREHENSIVE STUDY REPORT ===

{study_summary if study_summary else "Summary generation failed"}

=== END OF REPORT ==="""
            
            return final_report
            
        except Exception as e:
            show_error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ CT –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
            log_to_file(f"MedGemma CT study analysis error: {e}", "ERROR")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ CUDA
            if "CUDA" in str(e) or "NVML" in str(e):
                show_warning("üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ CUDA - –ø–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è...")
                log_to_file("CUDA error detected - attempting recovery...")
                
                # –û—á–∏—â–∞–µ–º –≤—Å—é CUDA –ø–∞–º—è—Ç—å
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    
                show_info("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                show_warning("   - –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—É")
                show_warning("   - –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –¥—Ä—É–≥–∏–µ GPU –ø—Ä–æ—Ü–µ—Å—Å—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø–∞–º—è—Ç—å")
                show_warning("   - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–∫–µ—Ç–∞")
                
            return None
    
    def analyze_medical_text(self, text: str, context: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é MedGemma
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ç MedGemma
        """
        if not self.model or not self.processor:
            show_error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            log_to_file("Model not loaded for text analysis")
            return None
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            if context:
                prompt = f"""Context: {context}

Medical Analysis Request: {text}

Please provide a detailed medical analysis including:
1. Clinical findings interpretation
2. Differential diagnosis considerations  
3. Recommended follow-up actions
4. Risk assessment

Analysis:"""
            else:
                prompt = f"""Medical Analysis Request: {text}

Please provide a detailed medical analysis including:
1. Clinical findings interpretation
2. Differential diagnosis considerations
3. Recommended follow-up actions  
4. Risk assessment

Analysis:"""
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are an expert medical AI assistant."}]
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }
            ]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True, 
                tokenize=True,
                return_dict=True, 
                return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16 if self.device != "cpu" else torch.float32)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs, 
                    max_new_tokens=512,
                    do_sample=False,
                    temperature=0.7 if self.device != "cpu" else 1.0
                )
                generation = generation[0][input_len:]
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.processor.decode(generation, skip_special_tokens=True)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            show_info("üîç –ü–û–õ–ù–´–ô –û–¢–í–ï–¢ MEDGEMMA (–¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑):")
            log_to_file("Full MedGemma response (Text Analysis):")
            log_to_file("=" * 50)
            log_to_file(response)
            log_to_file("=" * 50)
            
            return response.strip()
            
        except Exception as e:
            show_error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ MedGemma: {e}")
            log_to_file(f"MedGemma text analysis error: {e}", "ERROR")
            return None
    
    def analyze_radiology_finding(self, finding: str, image_context: str = "") -> Optional[str]:
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–æ–∫
        
        Args:
            finding: –†–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –Ω–∞—Ö–æ–¥–∫–∞
            image_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        prompt = f"""Radiology Finding: {finding}

{f"Image Context: {image_context}" if image_context else ""}

As a medical AI assistant specialized in radiology, please provide:

1. CLINICAL SIGNIFICANCE:
   - What does this finding indicate?
   - Severity assessment
   
2. DIFFERENTIAL DIAGNOSIS:
   - Most likely diagnoses
   - Alternative considerations
   
3. FOLLOW-UP RECOMMENDATIONS:
   - Additional imaging needed
   - Clinical correlation required
   - Urgent vs routine follow-up
   
4. PATIENT COUNSELING POINTS:
   - Key information for patient
   - Prognosis considerations"""
        
        return self.analyze_medical_text(prompt)


def test_medgemma():
    """–¢–µ—Å—Ç MedGemma –∫–ª–∏–µ–Ω—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"""
    show_step("üß™ –¢–ï–°–¢ MEDGEMMA –ö–õ–ò–ï–ù–¢–ê (–° –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú–ò)")
    log_to_file("Starting MedGemma client test (with images)")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
        client = MedGemmaClient()
        
        # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        show_info("\nüìä –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        log_to_file("\nüìä MedGemma image analysis test...")
        
        from image_processor import ImageProcessor
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if not os.path.exists("input"):
            show_error("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è 'input' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            log_to_file("Input directory 'input' not found")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_processor = ImageProcessor()
        images = image_processor.load_dicom_series("input")
        
        if not images:
            show_error("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            log_to_file("Images not found")
            return
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        test_image = images[0]
        show_info(f"üì∑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {test_image.get('filename', 'unknown')}")
        log_to_file(f"Testing on image: {test_image.get('filename', 'unknown')}")
        
        result = client.analyze_medical_image(test_image)
        
        if result:
            show_success(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
            log_to_file(f"\n‚úÖ Image analysis received (length: {len(result)} characters)")
            show_info(f"üìÑ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {result[:200]}...")
            log_to_file(f"üìÑ First 200 characters: {result[:200]}...")
        else:
            show_error("‚ùå –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –ø–æ–ª—É—á–µ–Ω")
            log_to_file("Image analysis not received")
        
        # –¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        show_info("\nüìù –¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        log_to_file("\nüìù MedGemma text analysis test...")
        test_finding = "CT –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∞–≤–æ–π –¥–æ–ª–µ –ø–µ—á–µ–Ω–∏"
        
        text_result = client.analyze_radiology_finding(test_finding)
        
        if text_result:
            show_success(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(text_result)} —Å–∏–º–≤–æ–ª–æ–≤)")
            log_to_file(f"‚úÖ Text analysis received (length: {len(text_result)} characters)")
            show_info(f"üìÑ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {text_result[:200]}...")
            log_to_file(f"üìÑ First 200 characters: {text_result[:200]}...")
        else:
            show_error("‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ –ø–æ–ª—É—á–µ–Ω")
            log_to_file("Text analysis not received")
            
    except Exception as e:
        show_error(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        log_to_file(f"Test error: {e}", "ERROR")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_medgemma() 