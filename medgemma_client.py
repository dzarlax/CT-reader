#!/usr/bin/env python3
"""
MedGemma Client
–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MedGemma 4B - —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –º–æ–¥–µ–ª—å—é –æ—Ç Google
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ —Ç–µ–∫—Å—Ç–∞
"""

import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import base64
from io import BytesIO
from typing import Optional, Dict, Any, List
import logging
import os
from dotenv import load_dotenv
from datetime import datetime

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
load_dotenv()

class MedGemmaClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è MedGemma 4B - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –º–æ–¥–µ–ª–∏ –æ—Ç Google —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, model_name: str = "google/medgemma-4b-it"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MedGemma –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            model_name: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ Hugging Face
        """
        self.model_name = model_name
        self.device = self._get_device()
        self.processor = None
        self.model = None
        
        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MedGemma –∫–ª–∏–µ–Ω—Ç–∞...")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if torch.cuda.is_available():
            self.device = "cuda"
            print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            # –û—á–∏—â–∞–µ–º CUDA –∫—ç—à –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤
            torch.cuda.empty_cache()
            print("üßπ CUDA –∫—ç—à –æ—á–∏—â–µ–Ω")
        elif torch.backends.mps.is_available():
            self.device = "mps"
            print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        else:
            self.device = "cpu"
            print(f"üì± –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å GPU
        if self.device == "cuda":
            # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã CUDA
            os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
            os.environ["CUDA_LAUNCH_BLOCKING"] = "1"
            print("üîß –ù–∞—Å—Ç—Ä–æ–π–∫–∏ CUDA –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω—É—é –ø–∞–º—è—Ç—å GPU
            if torch.cuda.is_available():
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
                gpu_memory_free = torch.cuda.memory_reserved(0) / 1024**3
                print(f"üíæ GPU –ø–∞–º—è—Ç—å: {gpu_memory:.1f}GB –æ–±—â–∞—è, {gpu_memory_free:.1f}GB —Å–≤–æ–±–æ–¥–Ω–∞—è")
                
                if gpu_memory < 8:
                    print("‚ö†Ô∏è –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ú–∞–ª–æ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è MedGemma. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 8GB")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
        self.token = os.getenv("HUGGINGFACE_TOKEN")
        if self.token:
            print("‚úÖ –¢–æ–∫–µ–Ω Hugging Face –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
            print(f"üîë –¢–æ–∫–µ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {self.token[:10]}...")
        else:
            print("‚ùå –¢–æ–∫–µ–Ω Hugging Face –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
            print("üí° –î–æ–±–∞–≤—å—Ç–µ HUGGINGFACE_TOKEN=your_token –≤ .env —Ñ–∞–π–ª")
            raise ValueError("–¢–æ–∫–µ–Ω Hugging Face –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MedGemma")
        
        self._load_model()
    
    def _get_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        if torch.cuda.is_available():
            return "cuda"
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            return "mps"  # Apple Silicon GPU
        else:
            return "cpu"
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä"""
        try:
            print(f"üì• –ó–∞–≥—Ä—É–∑–∫–∞ MedGemma –º–æ–¥–µ–ª–∏: {self.model_name}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ –∏ —Ç–µ–∫—Å—Ç–æ–º
            self.model = AutoModelForImageTextToText.from_pretrained(
                self.model_name,
                token=self.token,
                torch_dtype=torch.bfloat16 if self.device != "cpu" else torch.float32,
                device_map="auto" if self.device != "cpu" else None,
                trust_remote_code=True
            )
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                token=self.token,
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model = self.model.to(self.device)
            
            print(f"‚úÖ MedGemma –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            print(f"üîß –ú–æ–¥–µ–ª—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è + —Ç–µ–∫—Å—Ç")
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ MedGemma: {e}")
            print("üí° –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ:")
            print("   1. –£ –≤–∞—Å –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª–∏ google/medgemma-4b-it")
            print("   2. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è HUGGINGFACE_TOKEN")
            print("   3. –¢–æ–∫–µ–Ω –∏–º–µ–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞")
            print("   4. –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ accelerate: pip install accelerate")
            raise
    
    def analyze_medical_image(self, image_data: Dict[str, Any], prompt: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞–ø—Ä—è–º—É—é —Å –ø–æ–º–æ—â—å—é MedGemma
        
        Args:
            image_data: –î–∞–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è —Å base64_image
            prompt: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        """
        if not self.model or not self.processor:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
        
        try:
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏–∑ base64
            image_bytes = base64.b64decode(image_data['base64_image'])
            image = Image.open(BytesIO(image_bytes))
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if not prompt:
                prompt = """Analyze this medical CT image and provide a comprehensive medical assessment.

Please provide:

1. VISUAL FINDINGS:
   - Anatomical structures visible
   - Tissue densities and morphology
   - Any abnormal findings or pathology

2. CLINICAL INTERPRETATION:
   - Medical significance of findings
   - Differential diagnosis considerations
   - Severity assessment

3. RECOMMENDATIONS:
   - Additional imaging if needed
   - Clinical correlation required
   - Follow-up suggestions
   - Urgency level

Provide detailed, clinically relevant analysis focused on diagnostic and therapeutic implications."""
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are an expert radiologist specializing in CT image analysis."}]
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {"type": "image", "image": image}
                    ]
                }
            ]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True, 
                tokenize=True,
                return_dict=True, 
                return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16 if self.device != "cpu" else torch.float32)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ –¥–ª—è MedGemma
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=512,
                    do_sample=True,
                    top_p=0.9,
                    top_k=50,
                    repetition_penalty=1.1,
                    pad_token_id=self.processor.tokenizer.pad_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    use_cache=True
                )
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.processor.decode(outputs[0], skip_special_tokens=True)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            print("üîç –ü–û–õ–ù–´–ô –û–¢–í–ï–¢ MEDGEMMA (–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è):")
            print("=" * 50)
            print(response)
            print("=" * 50)
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∞–Ω–∞–ª–∏–∑–∞
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                torch.mps.empty_cache()
                
            return response.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è MedGemma: {e}")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø—Ä–∏ –æ—à–∏–±–∫–µ
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                torch.mps.empty_cache()
                
            return None
    
    def analyze_ct_study(self, images: List[Dict[str, Any]], study_context: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–µ—Ä–∏—é CT –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∫–∞–∫ –µ–¥–∏–Ω–æ–µ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ
        
        Args:
            images: –°–ø–∏—Å–æ–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            study_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            
        Returns:
            –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
        """
        if not images:
            return None
        
        print(f"üîç MedGemma –∞–Ω–∞–ª–∏–∑ CT –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è ({len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)...")
        print("üè• –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        
        try:
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –í–°–ï –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            individual_analyses = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
            batch_size = 5  # –£–º–µ–Ω—å—à–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è GPU
            total_batches = (len(images) + batch_size - 1) // batch_size
            
            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(images))
                batch_images = images[start_idx:end_idx]
                
                print(f"üì¶ –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–∫–µ—Ç–∞ {batch_idx + 1}/{total_batches} ({len(batch_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)...")
                
                # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø–∞–∫–µ—Ç–∞
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                elif self.device == "mps":
                    torch.mps.empty_cache()
                
                for i, image_data in enumerate(batch_images):
                    global_idx = start_idx + i + 1
                    print(f"üìä –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {global_idx}/{len(images)}: ", end="")
                    
                    slice_prompt = f"""Analyze this CT slice #{global_idx} from a medical study.

{f"Study Context: {study_context}" if study_context else ""}

Please provide:
1. Anatomical structures visible in this slice
2. Any pathological findings
3. Clinical significance of findings
4. Slice position and anatomical level

Focus on medically relevant observations. Be concise but thorough."""
                    
                    analysis = self.analyze_medical_image(image_data, slice_prompt)
                    
                    if analysis:
                        individual_analyses.append(f"=== CT SLICE {global_idx} ===\n{analysis}")
                        print("‚úÖ")
                    else:
                        print("‚ùå")
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ GPU
                if batch_idx < total_batches - 1:
                    print("‚è∏Ô∏è –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ GPU...")
                    import time
                    time.sleep(3)  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ø–∞—É–∑—É –¥–ª—è GPU
            
            if not individual_analyses:
                return None
            
            print(f"üìã –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—â–µ–≥–æ –æ—Ç—á—ë—Ç–∞ –∏–∑ {len(individual_analyses)} –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
            
            # –°–æ–∑–¥–∞—ë–º –æ–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è
            study_summary = self.analyze_medical_text(
                f"""Based on the following comprehensive CT study analysis, provide a detailed radiology report:

ANALYZED SLICES: {len(individual_analyses)} of {len(images)} total images

{chr(10).join(individual_analyses)}

Study Context: {study_context}

Please provide a structured radiology report including:

1. TECHNIQUE AND QUALITY
2. FINDINGS BY ANATOMICAL REGION
3. IMPRESSION/CONCLUSION
4. RECOMMENDATIONS

Format as a professional radiology report with detailed findings.""",
                "Complete CT study comprehensive analysis"
            )
            
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            final_report = f"""=== MEDGEMMA COMPLETE CT STUDY ANALYSIS ===

STUDY DETAILS:
- Total images in study: {len(images)}
- Images successfully analyzed: {len(individual_analyses)}
- Analysis date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- Model: MedGemma 4B (Google) - Vision + Text
- Analysis method: Direct image analysis
- Processing method: Batch processing for stability

=== INDIVIDUAL SLICE ANALYSES ===

{chr(10).join(individual_analyses)}

=== COMPREHENSIVE STUDY REPORT ===

{study_summary if study_summary else "Summary generation failed"}

=== END OF REPORT ==="""
            
            return final_report
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ CT –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è: {e}")
            
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—à–∏–±–æ–∫ CUDA
            if "CUDA" in str(e) or "NVML" in str(e):
                print("üîß –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –æ—à–∏–±–∫–∞ CUDA - –ø–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è...")
                
                # –û—á–∏—â–∞–µ–º –≤—Å—é CUDA –ø–∞–º—è—Ç—å
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
                    
                print("üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
                print("   - –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–æ–≥—Ä–∞–º–º—É")
                print("   - –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –¥—Ä—É–≥–∏–µ GPU –ø—Ä–æ—Ü–µ—Å—Å—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ø–∞–º—è—Ç—å")
                print("   - –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —É–º–µ–Ω—å—à–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –ø–∞–∫–µ—Ç–∞")
                
            return None
    
    def analyze_medical_text(self, text: str, context: str = "") -> Optional[str]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç —Å –ø–æ–º–æ—â—å—é MedGemma
        
        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ –æ—Ç MedGemma
        """
        if not self.model or not self.processor:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return None
        
        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –ø—Ä–æ–º–ø—Ç
            if context:
                prompt = f"""Context: {context}

Medical Analysis Request: {text}

Please provide a detailed medical analysis including:
1. Clinical findings interpretation
2. Differential diagnosis considerations  
3. Recommended follow-up actions
4. Risk assessment

Analysis:"""
            else:
                prompt = f"""Medical Analysis Request: {text}

Please provide a detailed medical analysis including:
1. Clinical findings interpretation
2. Differential diagnosis considerations
3. Recommended follow-up actions  
4. Risk assessment

Analysis:"""
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            messages = [
                {
                    "role": "system",
                    "content": [{"type": "text", "text": "You are an expert medical AI assistant."}]
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }
            ]
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            inputs = self.processor.apply_chat_template(
                messages, 
                add_generation_prompt=True, 
                tokenize=True,
                return_dict=True, 
                return_tensors="pt"
            ).to(self.model.device, dtype=torch.bfloat16 if self.device != "cpu" else torch.float32)
            
            input_len = inputs["input_ids"].shape[-1]
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            with torch.inference_mode():
                generation = self.model.generate(
                    **inputs, 
                    max_new_tokens=512,
                    do_sample=False,
                    temperature=0.7 if self.device != "cpu" else 1.0
                )
                generation = generation[0][input_len:]
            
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
            response = self.processor.decode(generation, skip_special_tokens=True)
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç–≤–µ—Ç
            print("üîç –ü–û–õ–ù–´–ô –û–¢–í–ï–¢ MEDGEMMA (–¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑):")
            print("=" * 50)
            print(response)
            print("=" * 50)
            
            return response.strip()
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ MedGemma: {e}")
            return None
    
    def analyze_radiology_finding(self, finding: str, image_context: str = "") -> Optional[str]:
        """
        –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–æ–∫
        
        Args:
            finding: –†–∞–¥–∏–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è –Ω–∞—Ö–æ–¥–∫–∞
            image_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            
        Returns:
            –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        """
        prompt = f"""Radiology Finding: {finding}

{f"Image Context: {image_context}" if image_context else ""}

As a medical AI assistant specialized in radiology, please provide:

1. CLINICAL SIGNIFICANCE:
   - What does this finding indicate?
   - Severity assessment
   
2. DIFFERENTIAL DIAGNOSIS:
   - Most likely diagnoses
   - Alternative considerations
   
3. FOLLOW-UP RECOMMENDATIONS:
   - Additional imaging needed
   - Clinical correlation required
   - Urgent vs routine follow-up
   
4. PATIENT COUNSELING POINTS:
   - Key information for patient
   - Prognosis considerations"""
        
        return self.analyze_medical_text(prompt)


def test_medgemma():
    """–¢–µ—Å—Ç MedGemma –∫–ª–∏–µ–Ω—Ç–∞ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏"""
    print("üß™ –¢–ï–°–¢ MEDGEMMA –ö–õ–ò–ï–ù–¢–ê (–° –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú–ò)")
    print("=" * 50)
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–ª–∏–µ–Ω—Ç
        client = MedGemmaClient()
        
        # –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        print("\nüìä –¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π...")
        
        from image_processor import ImageProcessor
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if not os.path.exists("input"):
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è 'input' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_processor = ImageProcessor()
        images = image_processor.load_dicom_series("input")
        
        if not images:
            print("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏
        test_image = images[0]
        print(f"üì∑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏: {test_image.get('filename', 'unknown')}")
        
        result = client.analyze_medical_image(test_image)
        
        if result:
            print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤)")
            print(f"üìÑ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {result[:200]}...")
        else:
            print("‚ùå –ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –ø–æ–ª—É—á–µ–Ω")
        
        # –¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        print("\nüìù –¢–µ—Å—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞...")
        test_finding = "CT –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ –≤ –ø—Ä–∞–≤–æ–π –¥–æ–ª–µ –ø–µ—á–µ–Ω–∏"
        
        text_result = client.analyze_radiology_finding(test_finding)
        
        if text_result:
            print(f"‚úÖ –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ–ª—É—á–µ–Ω (–¥–ª–∏–Ω–∞: {len(text_result)} —Å–∏–º–≤–æ–ª–æ–≤)")
            print(f"üìÑ –ü–µ—Ä–≤—ã–µ 200 —Å–∏–º–≤–æ–ª–æ–≤: {text_result[:200]}...")
        else:
            print("‚ùå –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –Ω–µ –ø–æ–ª—É—á–µ–Ω")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_medgemma() 