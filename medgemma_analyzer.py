#!/usr/bin/env python3
"""
MedGemma Analyzer - Google's Medical AI Model Integration
Provides direct medical image analysis using MedGemma model
"""

import os
import sys
import torch
import time
from typing import List, Dict, Any, Optional
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import config
from progress_logger import (
    show_step, show_success, show_error, show_info, show_warning, 
    start_progress, update_progress, complete_progress, 
    log_to_file, suppress_prints
)

# Check if MedGemma is available
try:
    from transformers import AutoProcessor, AutoModelForImageTextToText
    MEDGEMMA_AVAILABLE = True
    log_to_file("MedGemma dependencies available")
except ImportError as e:
    MEDGEMMA_AVAILABLE = False
    log_to_file(f"MedGemma –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}", "WARNING")

class MedGemmaAnalyzer:
    """Medical image analysis using Google's MedGemma model"""
    
    def __init__(self):
        """Initialize MedGemma analyzer"""
        if not MEDGEMMA_AVAILABLE:
            show_error("MedGemma –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
            
        try:
            show_step("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MedGemma –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")
            self.model_name = "google/medgemma-4b-it"
            self.device = self._get_device()
            self.processor = None
            self.model = None
            self.token = None
            self._load_model()
            show_success("MedGemma –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
            
        except Exception as e:
            show_error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ MedGemma: {e}")
            log_to_file(f"MedGemma initialization error: {e}", "ERROR")
            raise
    
    def analyze_study(self, images: List[Dict[str, Any]], user_context: str = "") -> Optional[str]:
        """
        Analyze CT study using MedGemma
        
        Args:
            images: List of processed image data
            user_context: Additional context from user
            
        Returns:
            Medical analysis text
        """
        if not images:
            show_error("–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
            return None
            
        show_step(f"–ó–∞–ø—É—Å–∫ MedGemma –∞–Ω–∞–ª–∏–∑–∞ ({len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)")
        log_to_file(f"Starting MedGemma analysis with {len(images)} images")
        
        if user_context:
            show_info(f"–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {user_context}")
        
        try:
            # Analyze all images with progress tracking
            result = self._analyze_ct_study(images, user_context)
            
            if result:
                show_success("MedGemma –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                log_to_file("MedGemma analysis completed successfully")
                return result
            else:
                show_warning("MedGemma –∞–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
                log_to_file("MedGemma analysis returned no results", "WARNING")
                return None
                
        except Exception as e:
            show_error(f"–û—à–∏–±–∫–∞ MedGemma –∞–Ω–∞–ª–∏–∑–∞: {e}")
            log_to_file(f"MedGemma analysis error: {e}", "ERROR")
            return None
    
    def _get_device(self):
        """Get the device for model loading"""
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            return "mps"
        else:
            return "cpu"
    
    def _setup_huggingface_token(self):
        """Setup Hugging Face token"""
        from dotenv import load_dotenv
        from huggingface_hub import login
        load_dotenv()
        
        self.token = os.getenv("HUGGINGFACE_TOKEN")
        if self.token:
            show_success("‚úÖ –¢–æ–∫–µ–Ω Hugging Face –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ")
            log_to_file("Hugging Face token found in .env file")
            show_info(f"üîë –¢–æ–∫–µ–Ω –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å: {self.token[:10]}...")
            log_to_file(f"Token starts with: {self.token[:10]}...")
            
            # –Ø–≤–Ω–∞—è –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ HuggingFace Hub
            try:
                login(token=self.token, add_to_git_credential=False)
                show_success("‚úÖ –ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è –≤ HuggingFace Hub —É—Å–ø–µ—à–Ω–∞")
                log_to_file("HuggingFace Hub authentication successful")
            except Exception as e:
                show_error(f"‚ùå –û—à–∏–±–∫–∞ –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏ –≤ HuggingFace Hub: {e}")
                log_to_file(f"HuggingFace Hub authentication error: {e}", "ERROR")
                raise
        else:
            show_error("‚ùå –¢–æ–∫–µ–Ω Hugging Face –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ .env —Ñ–∞–π–ª–µ!")
            show_info("üí° –î–æ–±–∞–≤—å—Ç–µ HUGGINGFACE_TOKEN=your_token –≤ .env —Ñ–∞–π–ª")
            log_to_file("Hugging Face token not found in .env file", "ERROR")
            raise ValueError("–¢–æ–∫–µ–Ω Hugging Face –æ–±—è–∑–∞—Ç–µ–ª–µ–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å MedGemma")
    
    def _load_model(self):
        """Load the MedGemma model and processor"""
        try:
            show_step("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ MedGemma")
            
            # Setup HuggingFace token
            self._setup_huggingface_token()
            
            # Load processor and model with token
            self.processor = AutoProcessor.from_pretrained(
                self.model_name,
                token=self.token,
                trust_remote_code=True
            )
            self.model = AutoModelForImageTextToText.from_pretrained(
                self.model_name,
                token=self.token,
                torch_dtype=torch.bfloat16 if self.device != "cpu" else torch.float32,
                device_map="auto" if self.device != "cpu" else None,
                trust_remote_code=True
            )
            
            if self.device == "cpu":
                self.model.to(self.device)
            
            show_success("–ú–æ–¥–µ–ª—å MedGemma –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            show_error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ MedGemma: {e}")
            log_to_file(f"Model loading error: {e}", "ERROR")
            raise
    
    def _analyze_ct_study(self, images: List[Dict[str, Any]], user_context: str = "") -> Optional[str]:
        """
        Analyze CT study using MedGemma
        
        Args:
            images: List of image dictionaries with 'base64_image', 'metadata', etc.
            user_context: Additional context from user
            
        Returns:
            Medical analysis text for the study
        """
        if not self.model or not self.processor:
            show_error("–ú–æ–¥–µ–ª—å MedGemma –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
            return None
            
        show_step(f"–ê–Ω–∞–ª–∏–∑ CT –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —Å {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏")
        log_to_file(f"Analyzing CT study with {len(images)} images")
        
        try:
            # Analyze first few images (limit for performance)
            max_images = min(5, len(images))  # Limit to 5 images for performance
            analyses = []
            
            for i in range(max_images):
                image_data = images[i]
                show_info(f"–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {i+1}/{max_images}")
                
                # Decode base64 image
                import base64
                from io import BytesIO
                image_bytes = base64.b64decode(image_data['base64_image'])
                image = Image.open(BytesIO(image_bytes))
                
                # Create medical prompt
                prompt = f"""Analyze this CT image #{i+1} from a medical study.

{f"Study Context: {user_context}" if user_context else ""}

Please provide:
1. Anatomical structures visible
2. Any pathological findings
3. Normal findings
4. Clinical significance
5. Recommendations

Focus on diagnostic and therapeutic implications."""
                
                # Prepare messages for MedGemma
                messages = [
                    {
                        "role": "system",
                        "content": [{"type": "text", "text": "You are an expert radiologist specializing in CT image analysis."}]
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image", "image": image}
                        ]
                    }
                ]
                
                # Process with MedGemma
                inputs = self.processor.apply_chat_template(
                    messages, 
                    add_generation_prompt=True, 
                    tokenize=True,
                    return_dict=True, 
                    return_tensors="pt"
                ).to(self.model.device, dtype=torch.bfloat16 if self.device != "cpu" else torch.float32)
                
                # Generate analysis
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=512,
                        do_sample=True,
                        top_p=0.9,
                        top_k=50,
                        repetition_penalty=1.1,
                        pad_token_id=self.processor.tokenizer.pad_token_id,
                        eos_token_id=self.processor.tokenizer.eos_token_id,
                        use_cache=True
                    )
                
                # Decode response
                response = self.processor.decode(outputs[0], skip_special_tokens=True)
                
                # Extract generated part (remove input prompt)
                generated_text = response.split("assistant")[-1].strip() if "assistant" in response else response.strip()
                
                if generated_text:
                    analyses.append(f"=== –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ï {i+1} ===\n{generated_text}")
                    show_success(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ {i+1} –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ")
                
                # Clear memory
                if self.device == "cuda":
                    torch.cuda.empty_cache()
                elif self.device == "mps":
                    torch.mps.empty_cache()
            
            # Combine all analyses
            if analyses:
                final_report = f"""=== MEDGEMMA –ê–ù–ê–õ–ò–ó CT –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø ===

–û–ë–©–ê–Ø –ò–ù–§–û–†–ú–ê–¶–ò–Ø:
- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(analyses)}
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: {user_context if user_context else "–ù–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω"}

{chr(10).join(analyses)}

=== –ö–û–ù–ï–¶ –ê–ù–ê–õ–ò–ó–ê ==="""
                
                show_success("MedGemma –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ")
                log_to_file("MedGemma analysis completed successfully")
                return final_report
            else:
                show_warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
                return None
            
        except Exception as e:
            show_error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            log_to_file(f"Image analysis error: {e}", "ERROR")
            
            # Clear memory on error
            if self.device == "cuda":
                torch.cuda.empty_cache()
            elif self.device == "mps":
                torch.mps.empty_cache()
                
            return None
    
    def _create_final_report(self, analyses: List[str]) -> str:
        """–°–æ–∑–¥–∞—ë—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á—ë—Ç"""
        
        report = f"""
=== MEDGEMMA –ú–ï–î–ò–¶–ò–ù–°–ö–ò–ô –ê–ù–ê–õ–ò–ó CT –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø ===

–î–ê–¢–ê –ê–ù–ê–õ–ò–ó–ê: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
–ö–û–õ–ò–ß–ï–°–¢–í–û –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô: {len(analyses)}
–ê–ù–ê–õ–ò–ó–ê–¢–û–†: MedGemma 4B (Google)

=== –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –ü–û –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø–ú ===

"""
        
        # –î–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã
        for analysis in analyses:
            report += analysis + "\n\n"
        
        # –°–æ–∑–¥–∞—ë–º –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ —Å –ø–æ–º–æ—â—å—é MedGemma
        if self.use_medgemma:
            try:
                summary_prompt = f"""Based on the following CT study analysis, provide a comprehensive summary:

{chr(10).join(analyses)}

Please provide:
1. OVERALL FINDINGS SUMMARY
2. KEY PATHOLOGICAL FINDINGS
3. CLINICAL SIGNIFICANCE
4. RECOMMENDATIONS
5. FOLLOW-UP SUGGESTIONS

Focus on the most clinically relevant findings and provide actionable recommendations."""
                
                summary = self.medgemma_client.analyze_medical_text(
                    summary_prompt,
                    "CT study comprehensive summary"
                )
                
                if summary:
                    report += f"""
=== –û–ë–©–ï–ï –†–ï–ó–Æ–ú–ï –ò–°–°–õ–ï–î–û–í–ê–ù–ò–Ø (MedGemma) ===

{summary}

=== –ö–û–ù–ï–¶ –û–¢–ß–Å–¢–ê ==="""
                
            except Exception as e:
                print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ä–µ–∑—é–º–µ: {e}")
                report += "\n=== –ö–û–ù–ï–¶ –û–¢–ß–Å–¢–ê ==="
        
        return report


def test_medgemma_analyzer():
    """–¢–µ—Å—Ç MedGemma –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
    print("üß™ –¢–ï–°–¢ MEDGEMMA –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê")
    print("=" * 50)
    
    try:
        from image_processor import ImageProcessor
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
        if not os.path.exists("input"):
            print("‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è 'input' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        image_processor = ImageProcessor()
        images = image_processor.load_dicom_series("input")
        
        if not images:
            print("‚ùå –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
            return
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ 2 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö
        test_images = images[:2]
        print(f"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ {len(test_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è—Ö")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = MedGemmaAnalyzer()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        result = analyzer.analyze_images(test_images)
        
        if result:
            print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à—ë–Ω —É—Å–ø–µ—à–Ω–æ!")
            print(f"üìÑ –î–ª–∏–Ω–∞ –æ—Ç—á—ë—Ç–∞: {len(result)} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"üîç –ü–µ—Ä–≤—ã–µ 300 —Å–∏–º–≤–æ–ª–æ–≤:")
            print(result[:300] + "...")
        else:
            print("‚ùå –ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∞: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_medgemma_analyzer() 